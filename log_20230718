➜  Abstract_trailcam_yolov6 git:(main) ✗ conda activate trailcam
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ nvidia-smi
Tue Jul 18 12:08:33 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |
| 64%   66C    P2    91W / 180W |   4767MiB /  8192MiB |     99%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:B3:00.0 Off |                  N/A |
|  0%   43C    P8     9W / 180W |      6MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2064      G   /usr/lib/xorg/Xorg                  9MiB |
|    0   N/A  N/A      2219      G   /usr/bin/gnome-shell                9MiB |
|    0   N/A  N/A   1121809      C   python                           4742MiB |
|    1   N/A  N/A      2064      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ kill -9 1121809
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ nvidia-smi     
Tue Jul 18 12:08:56 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |
| 65%   64C    P2    53W / 180W |   4767MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:B3:00.0 Off |                  N/A |
|  0%   43C    P8     8W / 180W |      6MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2064      G   /usr/lib/xorg/Xorg                  9MiB |
|    0   N/A  N/A      2219      G   /usr/bin/gnome-shell                9MiB |
|    1   N/A  N/A      2064      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ nvidia-smi
Tue Jul 18 12:09:08 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |
| 64%   61C    P0    49W / 180W |     21MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:B3:00.0 Off |                  N/A |
|  0%   43C    P8     8W / 180W |      6MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2064      G   /usr/lib/xorg/Xorg                  9MiB |
|    0   N/A  N/A      2219      G   /usr/bin/gnome-shell                9MiB |
|    1   N/A  N/A      2064      G   /usr/lib/xorg/Xorg                  4MiB |
+-----------------------------------------------------------------------------+
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 128 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160 -m torch.distributed.launch -nproc_per_node 8 --fuse_ab --device 0,1
2023-07-18 12:10:20.767328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:10:21.615351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: -m torch.distributed.launch -nproc_per_node 8

(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python -m torch.distributed.launch --nproc_per_node 8  tools/train.py --batch 256 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160 --fuse_ab --device 0,1
/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-18 12:12:18.679348: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:18.949596: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:18.972415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:18.972415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:19.058650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:19.059389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:19.070579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:19.561588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:19.607524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:19.891646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:19.909788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:19.909786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:20.474336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:20.505343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:20.515281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=1
2023-07-18 12:12:20.984732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=2
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=0
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=7
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127126 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127128 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127129 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127130 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127131 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127132 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127133 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 1 (pid: 1127127) of binary: /home/shared/.local/share/miniconda3/envs/trailcam/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-18_12:12:22
  host      : qaanaaq
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 1127127)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python -m torch.distributed.launch --nproc_per_node 8 tools/train.py --batch 128 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160 --fuse_ab --device 0,1
/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-18 12:12:43.562989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:43.562989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:43.562989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:43.864217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:44.008186: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:44.464636: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:44.501505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:44.502913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:44.503023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:44.517760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:44.517760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:12:44.806147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:45.416238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=1
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=4
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=5
2023-07-18 12:12:45.900029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:45.961700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:12:45.961700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=0
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127222 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127224 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127225 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127228 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127229 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 1 (pid: 1127223) of binary: /home/shared/.local/share/miniconda3/envs/trailcam/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-07-18_12:12:47
  host      : qaanaaq
  rank      : 4 (local_rank: 4)
  exitcode  : 2 (pid: 1127226)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-07-18_12:12:47
  host      : qaanaaq
  rank      : 5 (local_rank: 5)
  exitcode  : 2 (pid: 1127227)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-18_12:12:47
  host      : qaanaaq
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 1127223)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python -m torch.distributed.launch --nproc_per_node 8 tools/train.py --batch 256 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160 --fuse_ab --device 0,1
/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-18 12:13:15.698681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:15.883484: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:15.990756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:15.990797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:16.206154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:16.235689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:16.235689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:16.435712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:16.632829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:16.819684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:16.924993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:16.926931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:17.621639: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:17.673210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:17.673229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=5
2023-07-18 12:13:17.848827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=7
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=6
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=0
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127328 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127329 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127330 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127331 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127332 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1127334 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 5 (pid: 1127333) of binary: /home/shared/.local/share/miniconda3/envs/trailcam/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-07-18_12:13:19
  host      : qaanaaq
  rank      : 7 (local_rank: 7)
  exitcode  : 2 (pid: 1127335)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-18_12:13:19
  host      : qaanaaq
  rank      : 5 (local_rank: 5)
  exitcode  : 2 (pid: 1127333)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python -m torch.distributed.launch --nproc_per_node 2 tools/train.py --batch 256 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160 --fuse_ab --device 0,1
/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-07-18 12:13:29.725970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:29.732706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:30.604552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-07-18 12:13:30.605114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=1
usage: train.py [-h] [--data-path DATA_PATH] [--conf-file CONF_FILE] [--img-size IMG_SIZE] [--rect] [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--workers WORKERS] [--device DEVICE] [--eval-interval EVAL_INTERVAL] [--eval-final-only] [--heavy-eval-range HEAVY_EVAL_RANGE]
                [--check-images] [--check-labels] [--output-dir OUTPUT_DIR] [--name NAME] [--dist_url DIST_URL] [--gpu_count GPU_COUNT] [--local_rank LOCAL_RANK] [--resume [RESUME]] [--write_trainbatch_tb] [--stop_aug_last_n_epoch STOP_AUG_LAST_N_EPOCH]
                [--save_ckpt_on_last_n_epoch SAVE_CKPT_ON_LAST_N_EPOCH] [--distill] [--distill_feat] [--quant] [--calib] [--teacher_model_path TEACHER_MODEL_PATH] [--temperature TEMPERATURE] [--fuse_ab] [--bs_per_gpu BS_PER_GPU] [--specific-shape] [--height HEIGHT]
                [--width WIDTH]
train.py: error: unrecognized arguments: --local-rank=0
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 1127405) of binary: /home/shared/.local/share/miniconda3/envs/trailcam/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-07-18_12:13:33
  host      : qaanaaq
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 1127406)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-18_12:13:33
  host      : qaanaaq
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 1127405)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 256 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160                                  
2023-07-18 12:13:47.898697: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:13:48.742198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
--img-size 160 must be multiple of max stride 32, updating to 256
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=256, rect=False, batch_size=256, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp25')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 3828/ labels: 3828. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 303785.90it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 1092/ labels: 1092. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        3.2         0         0   0.08209: 100%|██████████| 15/15 [00:26<00:00,  1.74s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        3.2         0         0   0.01395: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99      3.199         0         0  0.007629: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99      3.197         0         0   0.00503: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      4/99      3.193         0         0  0.003552: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      5/99      3.188         0         0  0.002635: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      6/99       3.18         0         0  0.002006: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      7/99      3.172         0         0  0.001575: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      8/99      3.162         0         0  0.001281: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      9/99       3.15         0         0  0.001056: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     10/99      3.137         0         0 0.0008822: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     11/99      3.122         0         0 0.0007425: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     12/99      3.106         0         0 0.0006355: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     13/99      3.089         0         0 0.0005546: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     14/99       3.07         0         0 0.0004844: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     15/99      3.049         0         0 0.0004239: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     16/99      3.027         0         0 0.0003764: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     17/99      3.004         0         0 0.0003392: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     18/99      2.979         0         0 0.0003084:  67%|██████▋   | 10/15 [00:16<00:08,  1.63s/it]^C
^CTraceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 174, in train_in_steps
    self.scaler.scale(total_loss).backward()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
KeyboardInterrupt
^C
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf forImgClassifyCompressed/convertedLabels
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py                                                                                                           
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf forImgClassifyCompressed/convertedLabels
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py                       
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf forImgClassifyCompressed/convertedLabels
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py                       
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf forImgClassifyCompressed/convertedLabels
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py                       
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 256 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160                                                                      
2023-07-18 12:26:33.328406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:26:34.175359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
--img-size 160 must be multiple of max stride 32, updating to 256
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=256, rect=False, batch_size=256, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp26')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 131, in main
    trainer = Trainer(args, cfg, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 91, in __init__
    self.train_loader, self.val_loader = self.get_data_loader(self.args, self.cfg, self.data_dict)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 386, in get_data_loader
    train_loader = create_dataloader(train_path, args.img_size, args.batch_size // args.world_size, grid_size,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/data/data_load.py", line 46, in create_dataloader
    dataset = TrainValDataset(
              ^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/data/datasets.py", line 79, in __init__
    self.img_paths, self.labels = self.get_imgs_labels(self.img_dir)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/data/datasets.py", line 254, in get_imgs_labels
    assert osp.exists(img_dir), f"{img_dir} is an invalid directory path!"
AssertionError: ./custom_dataset/images/train is an invalid directory path!
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python datasetSpliter.py                                                                                                           
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 256 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 160
2023-07-18 12:26:51.769318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 12:26:52.615467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
--img-size 160 must be multiple of max stride 32, updating to 256
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=256, rect=False, batch_size=256, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp27')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [00:00<00:00, 22088.56it/s]
Train: Checking formats of labels with 8 process(es): 
0 label(s) found, 3828 label(s) missing, 0 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [00:00<00:00, 19571.07it/s]
WARNING: No labels found in ./custom_dataset/images/train. 
Train: Final numbers of valid images: 3828/ labels: 3828. 
1.1s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 11395.01it/s]
Val: Checking formats of labels with 8 process(es): 
0 label(s) found, 1092 label(s) missing, 0 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 10822.14it/s]
WARNING: No labels found in ./custom_dataset/images/val. 
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 87253.16it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 1092/ labels: 1092. 
1.4s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        3.2         0         0   0.08207: 100%|██████████| 15/15 [00:25<00:00,  1.68s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        3.2         0         0   0.01395: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99      3.199         0         0  0.007625: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99      3.197         0         0  0.005007: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      4/99      3.193         0         0  0.003551: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      5/99      3.188         0         0  0.002628: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      6/99       3.18         0         0  0.002032: 100%|██████████| 15/15 [00:22<00:00,  1.52s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      7/99      3.172         0         0  0.001591: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      8/99      3.162         0         0  0.001285: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      9/99       3.15         0         0  0.001045: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     10/99      3.137         0         0 0.0008705: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     11/99      3.122         0         0 0.0007372: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     12/99      3.106         0         0 0.0006322: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     13/99      3.089         0         0 0.0005476: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     14/99       3.07         0         0 0.0004788: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     15/99      3.049         0         0 0.0004228: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     16/99      3.027         0         0 0.0003724: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     17/99      3.004         0         0 0.0003356: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     18/99      2.979         0         0 0.0003026: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     19/99      2.953         0         0 0.0002747: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  1.82it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 19 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     20/99      2.926         0         0 0.0002513: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     21/99      2.897         0         0 0.0002302: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     22/99      2.868         0         0 0.0002132: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     23/99      2.836         0         0 0.0001962: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     24/99      2.804         0         0 0.0001821: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     25/99      2.771         0         0 0.0001698: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     26/99      2.736         0         0   0.00016: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     27/99        2.7         0         0 0.0001508: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     28/99      2.664         0         0 0.0001413: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     29/99      2.626         0         0 0.0001343: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     30/99      2.587         0         0 0.0001281: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     31/99      2.547         0         0 0.0001219: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     32/99      2.506         0         0  0.000116: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     33/99      2.465         0         0 0.0001116: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     34/99      2.422         0         0 0.0001071: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     35/99      2.379         0         0 0.0001022: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     36/99      2.335         0         0 9.833e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     37/99       2.29         0         0 9.523e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     38/99      2.245         0         0 9.202e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     39/99      2.199         0         0 8.898e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.07it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 39 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     40/99      2.153         0         0 8.642e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     41/99      2.105         0         0 8.446e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     42/99      2.058         0         0 8.208e-05: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     43/99       2.01         0         0 8.003e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     44/99      1.962         0         0 7.823e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     45/99      1.913         0         0 7.654e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     46/99      1.864         0         0 7.452e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     47/99      1.815         0         0 7.353e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     48/99      1.765         0         0 7.219e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     49/99      1.715         0         0 7.106e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     50/99      1.666         0         0 6.952e-05: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 50 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     51/99      1.616         0         0 6.864e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     52/99      1.566         0         0 6.761e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     53/99      1.517         0         0 6.659e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.20it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 53 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     54/99      1.467         0         0 6.585e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     55/99      1.417         0         0  6.49e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     56/99      1.368         0         0 6.397e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 56 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     57/99      1.319         0         0  6.33e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     58/99       1.27         0         0 6.274e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     59/99      1.222         0         0 6.194e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 59 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     60/99      1.174         0         0 6.162e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     61/99      1.127         0         0 6.117e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     62/99      1.079         0         0 6.062e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 62 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     63/99      1.033         0         0 6.028e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     64/99     0.9869         0         0 5.992e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     65/99     0.9416         0         0 5.944e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 65 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     66/99     0.8969         0         0 5.909e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     67/99     0.8529         0         0 5.869e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     68/99     0.8097         0         0 5.862e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 68 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     69/99     0.7673         0         0 5.819e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     70/99     0.7257         0         0 5.801e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     71/99     0.6849         0         0 5.772e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 71 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     72/99     0.6452         0         0 5.755e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     73/99     0.6063         0         0 5.744e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     74/99     0.5685         0         0 5.731e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.17it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 74 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     75/99     0.5317         0         0 5.711e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     76/99     0.4959         0         0 5.705e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     77/99     0.4613         0         0 5.692e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.16it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 77 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     78/99     0.4278         0         0 5.698e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     79/99     0.3955         0         0 5.691e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     80/99     0.3644         0         0  5.67e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.19it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 80 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     81/99     0.3345         0         0 5.677e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     82/99     0.3059         0         0 5.668e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     83/99     0.2786         0         0 5.666e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 83 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     84/99     0.2526         0         0 5.662e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 3828/ labels: 3828. 
0.1s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 460458.43it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 1092/ labels: 1092. 
0.0s for dataset initialization.

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     85/99     0.2279         0         0 5.671e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     86/99     0.2046         0         0 5.651e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.16it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 86 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     87/99     0.1828         0         0 5.663e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     88/99     0.1623         0         0 5.668e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     89/99     0.1432         0         0 5.649e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.21it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 89 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     90/99     0.1256         0         0 5.639e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     91/99     0.1095         0         0 5.658e-05: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     92/99    0.09489         0         0  5.66e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.21it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 92 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     93/99    0.08176         0         0 5.646e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     94/99    0.07015         0         0  5.65e-05: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     95/99    0.06006         0         0 5.645e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 95 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     96/99     0.0515         0         0 5.632e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     97/99    0.04449         0         0  5.62e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     98/99    0.03903         0         0 5.637e-05: 100%|██████████| 15/15 [00:22<00:00,  1.53s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.21it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 98 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     99/99    0.03513         0         0 5.638e-05: 100%|██████████| 15/15 [00:23<00:00,  1.54s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 3/3 [00:01<00:00,  2.22it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Epoch: 99 | mAP@0.5: 0.0 | mAP@0.50:0.95: 0.0

Training completed in 0.658 hours.
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ nvidia-smi                                                                                                                                                                                               
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/infer.py --weights runs/train/exp10/weights/best_ckpt.pt --source forImgClassifyCompressed/img/1/brno_00019.png --save-txt
Namespace(weights='runs/train/exp10/weights/best_ckpt.pt', source='forImgClassifyCompressed/img/1/brno_00019.png', webcam=False, webcam_addr='0', yaml='data/coco.yaml', img_size=[640, 640], conf_thres=0.4, iou_thres=0.45, max_det=1000, device='0', save_txt=True, not_save_img=False, save_dir=None, view_img=False, classes=None, agnostic_nms=False, project='runs/inference', name='exp', hide_labels=False, hide_conf=False, half=False)
Save directory already existed
Loading checkpoint from runs/train/exp10/weights/best_ckpt.pt

Fusing model...
Switch model to deploy modality.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.36it/s]
Results saved to runs/inference/exp
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ ll
drwxrwxr-x    - shared 17 Jul 17:21  assets/
drwxrwxr-x    - shared 17 Jul 17:21  configs/
drwxrwxr-x    - shared 18 Jul 12:26  custom_dataset/
drwxrwxr-x    - shared 17 Jul 17:56  data/
.rw-rw-r-- 3.6k shared 17 Jul 19:07  datasetSpliter.py
drwxrwxr-x    - shared 17 Jul 17:21  deploy/
drwxrwxr-x    - shared 17 Jul 17:21  docs/
drwxrwxr-x    - shared 18 Jul 12:25  forImgClassifyCompressed/
.rw-rw-r-- 7.3k shared 17 Jul 17:21  hubconf.py
.rw-rw-r-- 5.0M shared 17 Jul 17:21  inference.ipynb
.rw-rw-r-- 1.6k shared 18 Jul 12:25  labelConverter.py
.rw-rw-r--  35k shared 17 Jul 17:21  LICENSE
.rw-rw-r--  19k shared 17 Jul 17:21  README.md
.rw-rw-r--  18k shared 17 Jul 17:21  README_cn.md
.rw-rw-r--  341 shared 17 Jul 17:21  requirements.txt
drwxrwxr-x    - shared 18 Jul 11:16  runs/
drwxrwxr-x    - shared 18 Jul 11:09  tools/
.rw-rw-r-- 1.7M shared 17 Jul 17:21  turtorial.ipynb
drwxrwxr-x    - shared 17 Jul 17:40  yolov6/
.rw-rw-r-- 4.7M shared 17 Jul 17:40  yolov6lite_l.onnx
.rw-rw-r-- 2.6M shared 17 Jul 17:21  yolov6lite_l.pt
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ ll
drwxrwxr-x    - shared 17 Jul 17:21  assets/
drwxrwxr-x    - shared 17 Jul 17:21  configs/
drwxrwxr-x    - shared 18 Jul 12:26  custom_dataset/
drwxrwxr-x    - shared 17 Jul 17:56  data/
.rw-rw-r-- 3.6k shared 17 Jul 19:07  datasetSpliter.py
drwxrwxr-x    - shared 17 Jul 17:21  deploy/
drwxrwxr-x    - shared 17 Jul 17:21  docs/
drwxrwxr-x    - shared 18 Jul 12:25  forImgClassifyCompressed/
.rw-rw-r-- 7.3k shared 17 Jul 17:21  hubconf.py
.rw-rw-r-- 5.0M shared 17 Jul 17:21  inference.ipynb
.rw-rw-r-- 1.6k shared 18 Jul 12:25  labelConverter.py
.rw-rw-r--  35k shared 17 Jul 17:21  LICENSE
.rw-rw-r--  19k shared 17 Jul 17:21  README.md
.rw-rw-r--  18k shared 17 Jul 17:21  README_cn.md
.rw-rw-r--  341 shared 17 Jul 17:21  requirements.txt
drwxrwxr-x    - shared 18 Jul 11:16  runs/
drwxrwxr-x    - shared 18 Jul 11:09  tools/
.rw-rw-r-- 1.7M shared 17 Jul 17:21  turtorial.ipynb
drwxrwxr-x    - shared 17 Jul 17:40  yolov6/
.rw-rw-r-- 4.7M shared 17 Jul 17:40  yolov6lite_l.onnx
.rw-rw-r-- 2.6M shared 17 Jul 17:21  yolov6lite_l.pt
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ ps                                                                                                                                     
    PID TTY          TIME CMD
1125842 pts/7    00:00:00 bash
1125852 pts/7    00:00:07 zsh
1141622 pts/7    00:00:00 ps
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ cp -r ~/EuroCity-Persons-Dataset/forImgClassify .                      
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset                          
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py                                                                                                               
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python datasetSpliter.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 32 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640
2023-07-18 14:33:24.663565: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 14:33:25.508403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=32, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [00:00<00:00, 22676.43it/s]
Train: Checking formats of labels with 8 process(es): 
0 label(s) found, 3828 label(s) missing, 0 label(s) empty, 0 invalid label files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3828/3828 [00:00<00:00, 18890.90it/s]
WARNING: No labels found in ./custom_dataset/images/train. 
Train: Final numbers of valid images: 3828/ labels: 3828. 
0.9s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 9232.35it/s]
Val: Checking formats of labels with 8 process(es): 
0 label(s) found, 1092 label(s) missing, 0 label(s) empty, 0 invalid label files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 9950.25it/s]
WARNING: No labels found in ./custom_dataset/images/val. 
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1092/1092 [00:00<00:00, 384340.02it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 1092/ labels: 1092. 
1.1s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.4         0         0   0.01806: 100%|██████████| 120/120 [02:29<00:00,  1.25s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.4         0         0  0.002789: 100%|██████████| 120/120 [02:27<00:00,  1.23s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99     0.3999         0         0  0.001353: 100%|██████████| 120/120 [02:28<00:00,  1.23s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.3996         0         0 0.0007826: 100%|██████████| 120/120 [02:28<00:00,  1.23s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      4/99     0.3991         0         0 0.0004989: 100%|██████████| 120/120 [02:26<00:00,  1.22s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      5/99     0.3984         0         0 0.0003364: 100%|██████████| 120/120 [02:27<00:00,  1.23s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      6/99     0.3976         0         0 0.0002333: 100%|██████████| 120/120 [02:26<00:00,  1.22s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      7/99     0.3965         0         0 0.0001668: 100%|██████████| 120/120 [02:26<00:00,  1.22s/i

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      8/99     0.3952         0         0 0.0001361:  32%|███▏      | 38/120 [00:46<01:41,  1.23s/it^C
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 62, in __call__
    generate_anchors(feats, self.fpn_strides, self.grid_cell_size, self.grid_cell_offset, device=feats[0].device)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/anchor_generator.py", line 62, in generate_anchors
    stride_tensor = torch.cat(stride_tensor).to(device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 32 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640
2023-07-18 15:21:57.787916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:21:58.636272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=32, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp1')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:01<00:00, 991.58it/s]
Train: Checking formats of labels with 8 process(es): 
1400 label(s) found, 0 label(s) missing, 700 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:00<00:00, 19684.11it/s]
Train: Final numbers of valid images: 1400/ labels: 1400. 
2.1s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 418.76it/s]
Val: Checking formats of labels with 8 process(es): 
400 label(s) found, 0 label(s) missing, 200 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 8880.08it/s]
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 58596.03it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 400/ labels: 400. 
2.6s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/44 [00:00<?, ?it/s]                                                              OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4         0         0    0.2056:   2%|▏         | 1/44 [00:05<04:14,  5.92s/it] OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4         0         0    0.2056:   2%|▏         | 1/44 [00:08<06:15,  8.73s/it] 
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 81, in __call__
    self.warmup_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/atss_assigner.py", line 70, in forward
    is_in_gts = select_candidates_in_gts(ac_points, gt_bboxes)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 42, in select_candidates_in_gts
    bbox_deltas = torch.cat([b_lt, b_rb], dim=-1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 7.92 GiB total capacity; 6.42 GiB already allocated; 276.19 MiB free; 6.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 116, in __call__
    self.warmup_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/atss_assigner.py", line 82, in forward
    ious = iou_calculator(gt_bboxes, pd_bboxes) * mask_pos
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 83, in iou_calculator
    x2y2 = torch.minimum(px2y2, gx2y2)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C^C
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 32 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640
2023-07-18 15:22:44.456441: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:22:45.302655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=32, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp2')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1400/ labels: 1400. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 82536.61it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 400/ labels: 400. 
0.1s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/44 [00:00<?, ?it/s]                                                              OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
  0%|          | 0/44 [00:07<?, ?it/s]                                                              
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 176, in train_in_steps
    self.update_optimizer()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 370, in update_optimizer
    self.scaler.step(self.optimizer)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py", line 374, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py", line 289, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
               ^^^^^^^^
KeyboardInterrupt
^[[A^C^C
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640
2023-07-18 15:23:06.408488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:23:07.253176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=16, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp3')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1400/ labels: 1400. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 125035.15it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 400/ labels: 400. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.2         0         0   0.01796: 100%|██████████| 88/88 [01:02<00:00,  1.41it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.2         0         0  0.004281: 100%|██████████| 88/88 [00:58<00:00,  1.49it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99        0.2         0         0  0.002526: 100%|██████████| 88/88 [00:58<00:00,  1.52it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1998         0         0  0.001679: 100%|██████████| 88/88 [00:58<00:00,  1.52it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/88 [00:00<?, ?it/s]                                                              ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
  0%|          | 0/88 [00:00<?, ?it/s]                                                              
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 90, in __call__
    self.formal_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 69, in forward
    target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 59, in select_highest_overlaps
    if fg_mask.max() > 1:
       ^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 104, in __call__
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f50fb99e4d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f50fb96836b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f51016cffa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdfa365 (0x7f508b5fa365 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7f50c9ecd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7f50fb983e77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7f50fb97c69e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f50fb97c7b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7f50ca1530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7f50ca153415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #22: <unknown function> + 0x29d90 (0x7f5102c29d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x7f5102c29e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1161396 IOT instruction (core dumped)  python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 1
2023-07-18 15:27:34.857203: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:27:35.709625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=16, epochs=100, workers=8, device='1', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp4')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1400/ labels: 1400. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 82491.97it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 400/ labels: 400. 
0.1s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.2         0         0   0.01796: 100%|██████████| 88/88 [01:01<00:00,  1.43it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.2         0         0  0.004281: 100%|██████████| 88/88 [00:59<00:00,  1.48it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99        0.2         0         0  0.002526: 100%|██████████| 88/88 [00:58<00:00,  1.50it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1998         0         0  0.001679: 100%|██████████| 88/88 [00:58<00:00,  1.49it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/88 [00:00<?, ?it/s]                                                              ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [34,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [53,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
  0%|          | 0/88 [00:00<?, ?it/s]                                                              
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 90, in __call__
    self.formal_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 69, in forward
    target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 59, in select_highest_overlaps
    if fg_mask.max() > 1:
       ^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 104, in __call__
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f0562f9e4d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f0562f6836b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f0573f7cfa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdfa365 (0x7f0503bfa365 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7f05424cd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7f0562f83e77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7f0562f7c69e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f0562f7c7b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7f05427530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7f0542753415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #22: <unknown function> + 0x29d90 (0x7f057b029d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x7f057b029e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1162402 IOT instruction (core dumped)  python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 1
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf forImgClassify/convertedLabels          
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py                                                                                                                     
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python datasetSpliter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 1
2023-07-18 15:33:51.064409: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:33:51.922245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=16, epochs=100, workers=8, device='1', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp5')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:01<00:00, 1013.49it/s]
Train: Checking formats of labels with 8 process(es): 
1400 label(s) found, 0 label(s) missing, 700 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1400/1400 [00:00<00:00, 19380.97it/s]
Train: Final numbers of valid images: 1400/ labels: 1400. 
2.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 444.56it/s]
Val: Checking formats of labels with 8 process(es): 
400 label(s) found, 0 label(s) missing, 200 label(s) empty, 0 invalid label files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 13711.36it/s]
Convert to COCO format
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 119940.06it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 400/ labels: 400. 
1.7s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.2         0         0   0.01796: 100%|██████████| 88/88 [01:00<00:00,  1.44it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.2         0         0  0.004285: 100%|██████████| 88/88 [00:59<00:00,  1.47it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99        0.2         0         0  0.002525: 100%|██████████| 88/88 [00:59<00:00,  1.49it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1998         0         0  0.001676: 100%|██████████| 88/88 [00:58<00:00,  1.50it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/88 [00:00<?, ?it/s]                                                              ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [11,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
  0%|          | 0/88 [00:00<?, ?it/s]                                                              
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 90, in __call__
    self.formal_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 69, in forward
    target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 59, in select_highest_overlaps
    if fg_mask.max() > 1:
       ^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 104, in __call__
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 146, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 136, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f9f8359e4d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f9f8356836b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f9f945aefa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdfa365 (0x7f9f241fa365 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7f9f62acd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7f9f83583e77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7f9f8357c69e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f9f8357c7b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7f9f62d530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7f9f62d53415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #22: <unknown function> + 0x29d90 (0x7f9f9b829d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x7f9f9b829e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1163782 IOT instruction (core dumped)  python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 0
2023-07-18 15:39:59.882645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:40:00.726421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=16, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp6')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1400/ labels: 1400. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 134648.60it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 400/ labels: 400. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.2         0         0   0.01796: 100%|██████████| 88/88 [01:06<00:00,  1.32it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.2         0         0  0.004285: 100%|██████████| 88/88 [01:04<00:00,  1.36it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99        0.2         0         0  0.002525: 100%|██████████| 88/88 [01:03<00:00,  1.39it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1998         0         0  0.001676: 100%|██████████| 88/88 [01:03<00:00,  1.39it/s]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/88 [00:00<?, ?it/s]                                                              ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [32,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [33,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [34,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [35,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [36,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [37,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [38,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [39,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [40,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [41,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [42,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [43,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [44,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [45,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [46,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [47,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [48,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [49,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [50,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [51,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [52,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [53,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [54,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [55,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [56,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [57,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [58,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [59,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [60,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [61,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [62,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [63,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [3,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [1,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [2,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [3,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [4,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [5,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [6,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [7,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [8,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [9,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [10,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [11,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [12,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [13,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [14,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [15,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [16,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [17,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [18,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [19,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [20,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [21,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [22,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [23,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [24,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [25,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [26,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [27,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [28,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [29,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [30,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [31,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [64,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [65,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [66,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [67,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [68,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [69,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [70,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [71,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [72,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [73,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [74,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [75,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [76,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [77,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [78,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [79,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [80,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [81,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [82,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [83,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [84,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [85,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [86,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [87,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [88,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [89,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [90,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [91,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [92,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [93,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [94,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [15,0,0], thread: [95,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
  0%|          | 0/88 [00:00<?, ?it/s]                                                              
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 90, in __call__
    self.formal_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 66, in forward
    mask_pos, align_metric, overlaps = self.get_pos_mask(
                                       ^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 106, in get_pos_mask
    align_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 128, in get_box_metrics
    bbox_scores = pd_scores[ind[0], ind[1]]
                  ~~~~~~~~~^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 104, in __call__
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fe916c784d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7fe916c4236b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7fe916d1cfa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdfa365 (0x7fe8a0bfa365 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7fe8df4cd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7fe916c5de77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7fe916c5669e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7fe916c567b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7fe8df7530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7fe8df753415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #22: <unknown function> + 0x29d90 (0x7fe918229d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x7fe918229e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1164922 IOT instruction (core dumped)  python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python datasetSpliter.py                                                                                                                     
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset                
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python datasetSpliter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf forImgClassify/convertedLabels
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python labelConverter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset                
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python datasetSpliter.py                                                                                                                     
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset; rm -rf forImgClassify/convertedLabels; python labelConverter.py; python datasetSpliter.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset; rm -rf forImgClassify/convertedLabels; python labelConverter.py; python datasetSpliter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 0
2023-07-18 15:51:31.362449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 15:51:32.228995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=16, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp7')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1600/1600 [00:01<00:00, 1009.55it/s]
Train: Checking formats of labels with 8 process(es): 
1600 label(s) found, 0 label(s) missing, 800 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1600/1600 [00:00<00:00, 22828.71it/s]
Train: Final numbers of valid images: 1600/ labels: 1600. 
2.3s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 389.21it/s]
Val: Checking formats of labels with 8 process(es): 
200 label(s) found, 0 label(s) missing, 100 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 7486.29it/s]
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 87463.33it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
1.4s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.2         0         0   0.01652: 100%|██████████| 100/100 [01:15<00:00,  1.33it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.2         0         0  0.003801: 100%|██████████| 100/100 [01:13<00:00,  1.36it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99        0.2         0         0  0.002182: 100%|██████████| 100/100 [01:12<00:00,  1.38it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1998         0         0  0.001429: 100%|██████████| 100/100 [01:12<00:00,  1.38it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
  0%|          | 0/100 [00:00<?, ?it/s]                                                             ../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [96,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [97,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [98,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [99,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [100,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [101,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [102,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [103,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [104,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [105,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [106,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [107,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [108,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [109,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [110,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [111,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [112,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [113,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [114,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [115,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [116,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [117,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [118,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [119,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [120,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [121,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [122,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [123,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [124,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [125,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [126,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [60,0,0], thread: [127,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && "index out of bounds"` failed.
OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
  0%|          | 0/100 [00:00<?, ?it/s]                                                             
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 90, in __call__
    self.formal_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 66, in forward
    mask_pos, align_metric, overlaps = self.get_pos_mask(
                                       ^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 106, in get_pos_mask
    align_metric, overlaps = self.get_box_metrics(pd_scores, pd_bboxes, gt_labels, gt_bboxes)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/tal_assigner.py", line 128, in get_box_metrics
    bbox_scores = pd_scores[ind[0], ind[1]]
                  ~~~~~~~~~^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 104, in __call__
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7ff7b68c24d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7ff7b688c36b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7ff7b6966fa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdfa365 (0x7ff7407fa365 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7ff77f0cd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7ff7b68a7e77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7ff7b68a069e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7ff7b68a07b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7ff77f3530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7ff77f353415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #22: <unknown function> + 0x29d90 (0x7ff7b7e29d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x7ff7b7e29e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1166829 IOT instruction (core dumped)  python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset; rm -rf forImgClassify/convertedLabels; python labelConverter.py; python datasetSpliter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 0
2023-07-18 16:00:41.463539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:00:42.308885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=16, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp8')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1600/1600 [00:01<00:00, 1002.80it/s]
Train: Checking formats of labels with 8 process(es): 
1600 label(s) found, 0 label(s) missing, 800 label(s) empty, 0 invalid label files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1600/1600 [00:00<00:00, 19213.10it/s]
Train: Final numbers of valid images: 1600/ labels: 1600. 
2.3s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 380.10it/s]
Val: Checking formats of labels with 8 process(es): 
200 label(s) found, 0 label(s) missing, 100 label(s) empty, 0 invalid label files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 14321.14it/s]
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 65265.76it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
1.3s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.2     1.566         0     1.329: 100%|██████████| 100/100 [01:16<00:00,  1.31it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.2     1.398         0     1.353: 100%|██████████| 100/100 [01:13<00:00,  1.35it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99        0.2     1.303         0     1.339: 100%|██████████| 100/100 [01:13<00:00,  1.36it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1998     1.211         0     1.323: 100%|██████████| 100/100 [01:13<00:00,  1.36it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      4/99     0.1996     1.308         0      2.24: 100%|██████████| 100/100 [01:13<00:00,  1.37it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      5/99     0.1992     1.241         0     1.123: 100%|██████████| 100/100 [01:12<00:00,  1.38it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      6/99     0.1988     1.181         0     1.093: 100%|██████████| 100/100 [01:12<00:00,  1.38it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      7/99     0.1982     1.142         0     1.065: 100%|██████████| 100/100 [01:12<00:00,  1.38it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      8/99     0.1976     1.138         0      1.23: 100%|██████████| 100/100 [01:12<00:00,  1.37it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      9/99     0.1969     1.627         0 1.038e+04: 100%|██████████| 100/100 [01:12<00:00,  1.38it/

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     10/99     0.1961      1.67         0 1.479e+04:  22%|██▏       | 22/100 [00:16<00:57,  1.36it/s../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [22,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [25,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [27,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [213,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [64,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [65,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [66,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [67,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [68,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [69,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [70,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [71,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [72,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [73,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [74,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [75,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [76,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [77,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [78,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [79,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [80,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [81,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [82,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [83,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [84,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [85,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [86,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [87,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [88,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [89,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [90,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [91,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [92,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [93,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [94,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [95,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [80,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.
     10/99     0.1961      1.67         0 1.479e+04:  22%|██▏       | 22/100 [00:16<00:57,  1.35it/s
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 156, in __call__
    loss_cls = self.varifocal_loss(pred_scores, target_scores, one_hot_label)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 202, in forward
    loss = (F.binary_cross_entropy(pred_score.float(), gt_score.float(), reduction='none') * weight).sum()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/functional.py", line 3098, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f005d99e4d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7f005d96836b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7f006e97cfa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdfa365 (0x7efffe5fa365 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7f003cecd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7f005d983e77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7f005d97c69e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f005d97c7b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7f003d1530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7f003d153415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #22: <unknown function> + 0x29d90 (0x7f0075a29d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #23: __libc_start_main + 0x80 (0x7f0075a29e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1169703 IOT instruction (core dumped)  python tools/train.py --batch 16 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset; rm -rf forImgClassify/convertedLabels; python labelConverter.py; python datasetSpliter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ rm -rf custom_dataset; rm -rf forImgClassify/convertedLabels; python labelConverter.py; python datasetSpliter.py
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 24 --conf configs/yolov6_lite/yolov6_lite_l.py --d
ata data/EuroCity.yaml --epochs 100 --img-size 640 --device 0
2023-07-18 16:20:44.922642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:20:45.769825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=24, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp11')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|███████████████████████████████████████████████████████████████████| 1600/1600 [00:01<00:00, 1014.97it/s]
Train: Checking formats of labels with 8 process(es): 
1600 label(s) found, 0 label(s) missing, 800 label(s) empty, 0 invalid label files: 100%|████| 1600/1600 [00:00<00:00, 19274.30it/s]
Train: Final numbers of valid images: 1600/ labels: 1600. 
2.3s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Val: Checking formats of images with 8 process(es): 
0 image(s) corrupted: 100%|██████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 375.76it/s]
Val: Checking formats of labels with 8 process(es): 
200 label(s) found, 0 label(s) missing, 100 label(s) empty, 0 invalid label files: 100%|████████| 200/200 [00:00<00:00, 4742.73it/s]
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 62840.72it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
1.7s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.3     2.009         0     1.197:   4%|▍         | 3/67 [00:06<02:08,  2.01s/it] ^C
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 174, in train_in_steps
    self.scaler.scale(total_loss).backward()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
^C
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 32 --conf configs/yolov6_lite/yolov6_lite_l.py --d
ata data/EuroCity.yaml --epochs 100 --img-size 640 --device 0
2023-07-18 16:21:07.899939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:21:08.739924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=32, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp12')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1600/ labels: 1600. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 71906.46it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.4     2.022         0     1.179:   6%|▌         | 3/50 [00:07<01:41,  2.16s/it] OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4     2.022         0     1.179:   6%|▌         | 3/50 [00:09<02:36,  3.33s/it] 
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 81, in __call__
    self.warmup_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/atss_assigner.py", line 70, in forward
    is_in_gts = select_candidates_in_gts(ac_points, gt_bboxes)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 42, in select_candidates_in_gts
    bbox_deltas = torch.cat([b_lt, b_rb], dim=-1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 7.92 GiB total capacity; 6.42 GiB already allocated; 284.19 MiB free; 6.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 116, in __call__
    self.warmup_assigner(
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/atss_assigner.py", line 70, in forward
    is_in_gts = select_candidates_in_gts(ac_points, gt_bboxes)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/assigners/assigner_utils.py", line 44, in select_candidates_in_gts
    return (bbox_deltas.min(axis=-1)[0] > eps).to(gt_bboxes.dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 32 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 1
2023-07-18 16:21:26.847320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:21:27.698547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=32, epochs=100, workers=8, device='1', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='exp', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/exp13')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1600/ labels: 1600. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 73410.41it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.4     2.022         0     1.179:   6%|▌         | 3/50 [00:07<01:40,  2.14s/it] OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4     1.997         0     1.201:  10%|█         | 5/50 [00:12<01:45,  2.33s/it] OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4     1.965         0     1.225:  14%|█▍        | 7/50 [00:18<01:41,  2.35s/it] OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4       1.9         0     1.266:  22%|██▏       | 11/50 [00:26<01:12,  1.87s/it]OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      0/99        0.4     1.865         0      1.27:  28%|██▊       | 14/50 [00:34<01:28,  2.45s/it]^C
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 152, in train_in_steps
    preds, s_featmaps = self.model(images)
                        ^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/yolo_lite.py", line 39, in forward
    x = self.detect(x)
        ^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/heads/effidehead_lite.py", line 70, in forward
    x[i] = self.stems[i](x[i])
           ^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/layers/common.py", line 927, in forward
    x = self.act_1(self.bn_1(self.conv_dw_1(x)))
                             ^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 24 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 1 --name gpu0
2023-07-18 16:22:46.817987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:22:47.687653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=24, epochs=100, workers=8, device='1', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='gpu0', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/gpu0')

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 132, in main
    trainer = Trainer(args, cfg, device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 58, in __init__
    model = self.get_model(args, cfg, self.num_classes, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 413, in get_model
    model = build_lite_model(cfg, nc, device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/yolo_lite.py", line 81, in build_model
    model = Model(cfg, channels=3, num_classes=num_classes).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/yolo_lite.py", line 43, in _apply
    self = super()._apply(fn)
           ^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: out of memory
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 24 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 0 --name gpu0
2023-07-18 16:23:00.891606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:23:01.786060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=24, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='gpu0', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/gpu01')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1600/ labels: 1600. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 71041.73it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99        0.3     1.617         0     1.332: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99        0.3     1.412         0      1.36: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99     0.2999     1.326         0     1.324: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.2997     1.233         0     1.296: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      4/99     0.2993     1.042         0     1.086: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      5/99     0.2988     1.018         0     1.194: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      6/99     0.2982     1.376         0      1.41: 100%|██████████| 67/67 [01:11<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      7/99     0.2974     1.278         0     1.181:  57%|█████▋    | 38/67 [00:40<00:30,  1.05s/it]OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      7/99     0.2974     1.262         0     1.145: 100%|██████████| 67/67 [01:13<00:00,  1.10s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      8/99     0.2964     1.161         0     1.061: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      9/99     0.2953     1.124         0     1.029: 100%|██████████| 67/67 [01:11<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     10/99     0.2941     1.094         0     1.024: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     11/99     0.2927     1.056         0    0.9986: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     12/99     0.2912     1.071         0    0.9894: 100%|██████████| 67/67 [01:11<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     13/99     0.2896     1.107         0    0.9594: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     14/99     0.9708:     14/99     0.9708:     14/99        0     | 16/67 [     14/99        0     | 17/67 [     14/99        0     | 17/67 [     14/99        0     | 18/67 [     14/99     0.2878     1.149         0    0.9767:  27%|██▋       | 18/67 [00:20<00:51,  1.05s/it     14/99     0.2878     1.149         0    0.9767:  28%|██▊       | 19/67 [00:20<00:50,  1.05s/it     14/99     0.2878     1.156         0    0.9793:  28%|██▊       | 19/67 [00:21<00:50,  1.05s/it     14/99     0.2878     1.156         0    0.9793:  30%|██▉       | 20/67 [00:21<00:49,  1.05s/it     14/99     0.2878     1.158         0    0.9765:  30%|██▉       | 20/67 [00:22<00:49,  1.05s/it     14/99     0.2878     1.158         0    0.9765:  31%|███▏      | 21/67 [00:22<00:48,  1.05s/it     14/99     0.2878      1.16         0    0.9755:  31%|███▏      | 21/67 [00:23<00:48,  1.05s/it     14/99     0.2878      1.16         0    0.9755:  33%|███▎      | 22/67 [00:23<00:47,  1.07s/it     14/99     0.2878     1.159         0    0.9763:  33%|███▎      | 22/67 [00:24<00:47,  1.07s/it     14/99     0.2878     1.159         0    0.9763:  34%|███▍      | 23/67 [00:24<00:46,  1.06s/it     14/99     0.2878      1.16         0    0.9747:  34%|███▍      | 23/67 [00:25<00:46,  1.06s/it     14/99     0.2878      1.16         0    0.9747:  36%|███▌      | 24/67 [00:25<00:45,  1.05s/it     14/99     0.2878     1.159         0    0.9749:  36%|███▌      | 24/67 [00:26<00:45,  1.05s/it     14/99     0.2878     1.159         0    0.9749:  37%|███▋      | 25/67 [00:26<00:44,  1.05s/it     14/99     0.2878     1.158         0    0.9768:  37%|███▋      | 25/67 [00:27<00:44,  1.05s/it     14/99     0.2878     1.158         0    0.9768:  39%|███▉      | 26/67 [00:27<00:42,  1.04s/it     14/99     0.2878     1.155         0    0.9793:  39%|███▉      | 26/67 [00:28<00:42,  1.04s/it     14/99     0.2878     1.155         0    0.9793:  40%|████      | 27/67 [00:28<00:41,  1.04s/it     14/99     0.2878      1.15         0     1.045: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     15/99     0.2859     1.455         0      3543: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     16/99     0.2838     1.526         0      4357:  16%|█▋        | 11/67 [00:11<00:59,  1.06s/it]../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [64,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [65,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [66,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [67,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [68,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [69,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [70,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [71,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [72,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [73,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [74,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [75,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [76,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [77,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [78,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [79,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [80,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [81,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [82,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [83,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [84,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [85,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [86,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [87,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [88,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [89,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [90,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [91,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [92,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [93,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [94,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [72,0,0], thread: [95,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [22,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [25,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [27,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [60,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [32,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [33,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [34,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [35,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [36,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [37,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [38,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [39,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [40,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [41,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [42,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [43,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [44,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [45,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [46,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [47,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [48,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [49,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [50,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [51,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [52,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [53,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [54,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [55,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [56,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [57,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [58,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [59,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [60,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [61,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [62,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [41,0,0], thread: [63,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [53,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [32,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [33,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [34,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [35,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [36,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [37,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [38,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [39,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [40,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [41,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [42,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [43,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [44,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [45,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [46,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [47,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [48,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [49,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [50,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [51,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [52,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [53,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [54,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [55,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [56,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [57,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [58,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [59,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [60,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [61,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [62,0,0] Assertion `input_val >= zero && input_val <= one` failed.
../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [110,0,0], thread: [63,0,0] Assertion `input_val >= zero && input_val <= one` failed.
     16/99     0.2838     1.526         0      4357:  16%|█▋        | 11/67 [00:12<01:02,  1.12s/it]
ERROR in training steps.
ERROR in training loop or eval/save model.
Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 121, in train
    self.train_one_epoch(self.epoch)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 135, in train_one_epoch
    self.train_in_steps(epoch_num, self.step)
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 169, in train_in_steps
    total_loss, loss_items = self.compute_loss(preds, targets, epoch_num, step_num,
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 156, in __call__
    loss_cls = self.varifocal_loss(pred_scores, target_scores, one_hot_label)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/models/losses/loss.py", line 202, in forward
    loss = (F.binary_cross_entropy(pred_score.float(), gt_score.float(), reduction='none') * weight).sum()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/nn/functional.py", line 3098, in binary_cross_entropy
    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 147, in <module>
    main(args)
  File "/home/shared/Abstract_trailcam_yolov6/tools/train.py", line 137, in main
    trainer.train()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 129, in train
    self.train_after_loop()
  File "/home/shared/Abstract_trailcam_yolov6/yolov6/core/engine.py", line 357, in train_after_loop
    torch.cuda.empty_cache()
  File "/home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/cuda/memory.py", line 133, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:44 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7fea7f4634d7 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x7fea7f42d36b in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x7fea7f507fa8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0xdf9cde (0x7fea093f9cde in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x4cd0b6 (0x7fea47ccd0b6 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #5: <unknown function> + 0x3ee77 (0x7fea7f448e77 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #6: c10::TensorImpl::~TensorImpl() + 0x1be (0x7fea7f44169e in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7fea7f4417b9 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: <unknown function> + 0x7530c8 (0x7fea47f530c8 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #9: THPVariable_subclass_dealloc(_object*) + 0x2c5 (0x7fea47f53415 in /home/shared/.local/share/miniconda3/envs/trailcam/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
frame #26: <unknown function> + 0x29d90 (0x7fea80a29d90 in /lib/x86_64-linux-gnu/libc.so.6)
frame #27: __libc_start_main + 0x80 (0x7fea80a29e40 in /lib/x86_64-linux-gnu/libc.so.6)

[1]    1178200 IOT instruction (core dumped)  python tools/train.py --batch 24 --conf configs/yolov6_lite/yolov6_lite_l.py 
(trailcam) ➜  Abstract_trailcam_yolov6 git:(main) ✗ python tools/train.py --batch 24 --conf configs/yolov6_lite/yolov6_lite_l.py --data data/EuroCity.yaml --epochs 100 --img-size 640 --device 0 --name gpu1
2023-07-18 16:44:11.889932: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:44:12.788998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Using 1 GPU for training... 
training args are: Namespace(data_path='data/EuroCity.yaml', conf_file='configs/yolov6_lite/yolov6_lite_l.py', img_size=640, rect=False, batch_size=24, epochs=100, workers=8, device='0', eval_interval=20, eval_final_only=False, heavy_eval_range=50, check_images=False, check_labels=False, output_dir='./runs/train', name='gpu1', dist_url='env://', gpu_count=0, local_rank=-1, resume=False, write_trainbatch_tb=False, stop_aug_last_n_epoch=15, save_ckpt_on_last_n_epoch=-1, distill=False, distill_feat=False, quant=False, calib=False, teacher_model_path=None, temperature=20, fuse_ab=False, bs_per_gpu=32, specific_shape=False, height=None, width=None, rank=-1, world_size=1, save_dir='runs/train/gpu1')

Model: Model(
  (backbone): Lite_EffiBackbone(
    (conv_0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (lite_effiblock_1): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(12, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=12, bias=False)
            (bn): BatchNorm2d(12, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(12, 3, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(3, 12, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(12, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_2): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)
            (bn): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_3): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (3): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (4): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (5): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (6): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
    (lite_effiblock_4): Sequential(
      (0): Lite_EffiBlockS2(
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_2): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_2): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_pw_3): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (1): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
      (2): Lite_EffiBlockS1(
        (conv_pw_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_dw_1): ConvBN(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          )
        )
        (se): SEBlock(
          (avg_pool): AdaptiveAvgPool2d(output_size=1)
          (conv1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
          (hardsigmoid): Hardsigmoid()
        )
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
      )
    )
  )
  (neck): Lite_EffiNeck(
    (reduce_layer0): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer1): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (reduce_layer2): ConvBNHS(
      (block): ConvModule(
        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act): Hardswish()
      )
    )
    (upsample0): Upsample(scale_factor=2.0, mode='nearest')
    (upsample1): Upsample(scale_factor=2.0, mode='nearest')
    (Csp_p4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_p3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n3): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (Csp_n4): CSPBlock(
      (conv_1): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_2): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (conv_3): ConvBNHS(
        (block): ConvModule(
          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act): Hardswish()
        )
      )
      (blocks): DarknetBlock(
        (conv_1): ConvBNHS(
          (block): ConvModule(
            (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
            (act): Hardswish()
          )
        )
        (conv_2): DPBlock(
          (conv_dw_1): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
          (bn_1): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_1): Hardswish()
          (conv_pw_1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1))
          (bn_2): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
          (act_2): Hardswish()
        )
      )
    )
    (downsample2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (downsample1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_1): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
    (p6_conv_2): DPBlock(
      (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96)
      (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_1): Hardswish()
      (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
      (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
      (act_2): Hardswish()
    )
  )
  (detect): Detect(
    (stems): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (reg_convs): ModuleList(
      (0-3): 4 x DPBlock(
        (conv_dw_1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (bn_1): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_1): Hardswish()
        (conv_pw_1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))
        (bn_2): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        (act_2): Hardswish()
      )
    )
    (cls_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
    )
    (reg_preds): ModuleList(
      (0-3): 4 x Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1600/ labels: 1600. 
0.0s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|█████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 116443.75it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
0.0s for dataset initialization.
Training start...

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      0/99       0.15     1.624         0     1.333: 100%|██████████| 67/67 [01:14<00:00,  1.12s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      1/99       0.15     1.424         0     1.371: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      2/99       0.15     1.373         0     1.328: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      3/99     0.1499     1.294         0     1.323: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      4/99     0.1497     1.105         0     1.105: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      5/99     0.1494     1.041         0     1.153: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      6/99     0.1491    0.9918         0     1.094: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      7/99     0.1487    0.9813         0      1.08:  57%|█████▋    | 38/67 [00:40<00:30,  1.05s/it]OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
      7/99     0.1487    0.9791         0     1.086: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      8/99     0.1482    0.9471         0     1.065: 100%|██████████| 67/67 [01:12<00:00,  1.08s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
      9/99     0.1477    0.9257         0     1.061: 100%|██████████| 67/67 [01:12<00:00,  1.08s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     10/99     0.1471    0.9132         0     1.056: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     11/99     0.1464     0.884         0      1.04: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     12/99     0.1456    0.8772         0     1.033: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     13/99     0.1448    0.8742         0     1.027: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     14/99     0.1439    0.8839         0     1.048: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     15/99     0.1429    0.8634         0     1.094: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     16/99     0.1419    0.8467         0     1.001: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     17/99     0.1408    0.8343         0    0.9901: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]^[[C

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     18/99     0.1397    0.8046         0    0.9821: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     19/99     0.1384    0.8019         0     0.971: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.34it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.34s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.09s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.098
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.273
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.236
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.040
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.182
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.486
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.565
Results saved to runs/train/gpu1
Epoch: 19 | mAP@0.5: 0.2727819131825666 | mAP@0.50:0.95: 0.0977611244493726

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     20/99     0.1372    0.7935         0     0.965: 100%|██████████| 67/67 [01:11<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     21/99     0.1358    0.7701         0    0.9565: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     22/99     0.1344    0.7653         0    0.9483: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     23/99      0.133    0.7636         0    0.9374: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     24/99     0.1314    0.7372         0    0.9268: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     25/99     0.1299    0.7392         0    0.9117:  72%|███████▏  | 48/67 [00:50<00:19,  1.05s/it]OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
     25/99     0.1299    0.7361         0    0.9154: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     26/99     0.1283    0.7433         0    0.9219: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     27/99     0.1266    0.7453         0    0.9117: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     28/99     0.1249    0.7281         0    0.9011: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     29/99     0.1231    0.7161         0    0.9005: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     30/99     0.1213    0.7118         0    0.8874: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     31/99     0.1194    0.7129         0    0.8917: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     32/99     0.1175    0.6966         0    0.8905: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     33/99     0.1155    0.6954         0    0.8784: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     34/99     0.1135    0.6965         0    0.8743: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     35/99     0.1115    0.6921         0    0.8725: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     36/99     0.1095     0.689         0    0.8725: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     37/99     0.1074    0.6806         0    0.8633: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     38/99     0.1052    0.6708         0    0.8619: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     39/99     0.1031    0.6802         0    0.8606: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.58it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.02s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.465
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.198
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.440
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.467
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.358
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.275
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.562
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.545
Results saved to runs/train/gpu1
Epoch: 39 | mAP@0.5: 0.46495360645629624 | mAP@0.50:0.95: 0.2270332831276166

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     40/99     0.1009    0.6754         0    0.8522: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     41/99    0.09869    0.6648         0    0.8583: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     42/99    0.09647      0.66         0    0.8438: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     43/99    0.09422    0.6606         0    0.8444: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     44/99    0.09195    0.6558         0    0.8445: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     45/99    0.08966    0.6592         0    0.8467: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     46/99    0.08737    0.6489         0    0.8389: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     47/99    0.08506    0.6437         0    0.8313: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     48/99    0.08274    0.6404         0     0.835: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     49/99    0.08041    0.6396         0    0.8291: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     50/99    0.07808    0.6388         0    0.8247: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.46it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.03s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.502
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.472
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.540
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.072
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.376
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.616
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.635
Results saved to runs/train/gpu1
Epoch: 50 | mAP@0.5: 0.5024401529453031 | mAP@0.50:0.95: 0.2491719223420922

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     51/99    0.07575    0.6367         0    0.8246: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     52/99    0.07342    0.6323         0    0.8221: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     53/99    0.07109    0.6399         0    0.8328: 100%|██████████| 67/67 [01:11<00:00,  1.07s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.46it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.34s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.05s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.258
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.520
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.234
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.477
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.075
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.265
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.382
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.290
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.600
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.675
Results saved to runs/train/gpu1
Epoch: 53 | mAP@0.5: 0.5200630752441006 | mAP@0.50:0.95: 0.258306095445661

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     54/99    0.06876    0.6347         0    0.8191: 100%|██████████| 67/67 [01:17<00:00,  1.15s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     55/99    0.06644    0.6251         0      0.82: 100%|██████████| 67/67 [01:20<00:00,  1.19s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     56/99    0.06413    0.6224         0    0.8169: 100%|██████████| 67/67 [01:19<00:00,  1.18s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.26it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.02s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.516
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.235
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.163
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.480
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.074
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.314
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.602
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.695
Results saved to runs/train/gpu1
Epoch: 56 | mAP@0.5: 0.5163658559020478 | mAP@0.50:0.95: 0.26176741971808964

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     57/99    0.06184    0.6266         0    0.8218: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     58/99    0.05955    0.6198         0    0.8134: 100%|██████████| 67/67 [01:18<00:00,  1.17s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     59/99    0.05728    0.6196         0    0.8133: 100%|██████████| 67/67 [01:18<00:00,  1.18s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:04<00:00,  1.20it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.49s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.91s).
Accumulating evaluation results...
DONE (t=0.19s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.548
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.252
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.180
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.495
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.587
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.076
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.275
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.401
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.310
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.616
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680
Results saved to runs/train/gpu1
Epoch: 59 | mAP@0.5: 0.5479860835545675 | mAP@0.50:0.95: 0.275693296830177

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     60/99    0.05503     0.616         0    0.8057: 100%|██████████| 67/67 [01:13<00:00,  1.09s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     61/99    0.05281     0.612         0    0.8091: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     62/99     0.0506    0.6122         0    0.8111: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.53it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.33s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.02s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.273
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.536
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.247
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.175
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.487
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.565
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.076
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.594
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.655
Results saved to runs/train/gpu1
Epoch: 62 | mAP@0.5: 0.5363279465382055 | mAP@0.50:0.95: 0.2725110166139033

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     63/99    0.04842    0.6075         0    0.8015:  97%|█████████▋| 65/67 [01:08<00:02,  1.07s/it]OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
     63/99    0.04842    0.6074         0    0.8019: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     64/99    0.04626    0.6062         0    0.8032: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     65/99    0.04414    0.6047         0    0.8086: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.70it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.00s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.539
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.256
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.516
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.638
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.077
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.631
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.710
Results saved to runs/train/gpu1
Epoch: 65 | mAP@0.5: 0.5391017474039184 | mAP@0.50:0.95: 0.27996735422470803

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     66/99    0.04204    0.6044         0    0.7989: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     67/99    0.03998      0.61         0    0.8022: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     68/99    0.03795    0.5984         0    0.8042: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.69it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.99s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.542
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.181
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.511
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.634
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.283
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.409
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.631
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.700
Results saved to runs/train/gpu1
Epoch: 68 | mAP@0.5: 0.542455173588665 | mAP@0.50:0.95: 0.2843704044247524

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     69/99    0.03596     0.606         0     0.797: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     70/99    0.03402    0.6034         0    0.7958: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     71/99    0.03211    0.5966         0    0.7941: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.70it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.30s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.08s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.545
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.258
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.504
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.662
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.282
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.414
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.331
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.606
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.715
Results saved to runs/train/gpu1
Epoch: 71 | mAP@0.5: 0.5449383261311863 | mAP@0.50:0.95: 0.28009199096840415

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     72/99    0.03024    0.5987         0    0.7988: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     73/99    0.02842    0.5978         0    0.7949: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     74/99    0.02665    0.5914         0    0.7866: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.64it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.00s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.551
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.255
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.185
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.507
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.286
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.414
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.329
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.613
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.715
Results saved to runs/train/gpu1
Epoch: 74 | mAP@0.5: 0.5509340027867489 | mAP@0.50:0.95: 0.28667471520966004

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     75/99    0.02492     0.587         0    0.7903:  72%|███████▏  | 48/67 [00:50<00:20,  1.06s/it]OOM RuntimeError is raised due to the huge memory cost during label assignment.                     CPU mode is applied in this batch. If you want to avoid this issue,                     try to reduce the batch size or image size.
------------CPU Mode for This Batch-------------
     75/99    0.02492    0.5907         0    0.7916: 100%|██████████| 67/67 [01:14<00:00,  1.11s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     76/99    0.02325    0.5904         0    0.7868: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     77/99    0.02162    0.5902         0    0.7845: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.72it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.29s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.99s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.555
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.189
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.511
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.655
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.415
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.327
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.620
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.720
Results saved to runs/train/gpu1
Epoch: 77 | mAP@0.5: 0.5551699240531675 | mAP@0.50:0.95: 0.2908680512664199

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     78/99    0.02005    0.5941         0    0.7886: 100%|██████████| 67/67 [01:11<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     79/99    0.01854    0.5857         0    0.7762: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     80/99    0.01708    0.5837         0    0.7822: 100%|██████████| 67/67 [01:10<00:00,  1.06s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.72it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.99s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.562
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.192
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.516
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.618
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.079
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.291
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.410
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.624
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.685
Results saved to runs/train/gpu1
Epoch: 80 | mAP@0.5: 0.5616393342473195 | mAP@0.50:0.95: 0.2914449332293312

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     81/99    0.01568    0.5812         0     0.782: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     82/99    0.01434    0.5807         0    0.7807: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     83/99    0.01306     0.579         0    0.7843: 100%|██████████| 67/67 [01:10<00:00,  1.05s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.70it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.31s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.98s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.296
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.572
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.265
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.194
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.520
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.625
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.293
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.419
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.329
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.628
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.720
Results saved to runs/train/gpu1
Epoch: 83 | mAP@0.5: 0.5716996492689814 | mAP@0.50:0.95: 0.2956040697324243

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     84/99    0.01184    0.5767         0    0.7761: 100%|██████████| 67/67 [01:11<00:00,  1.06s/it]
img record infomation path is:./custom_dataset/images/.train_cache.json
Train: Final numbers of valid images: 1600/ labels: 1600. 
0.1s for dataset initialization.
img record infomation path is:./custom_dataset/images/.val_cache.json
Convert to COCO format
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 52871.60it/s]
Convert to COCO format finished. Resutls saved in ./custom_dataset/annotations/instances_val.json
Val: Final numbers of valid images: 200/ labels: 200. 
0.0s for dataset initialization.

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     85/99    0.01068    0.5858         0    0.7959: 100%|██████████| 67/67 [01:17<00:00,  1.16s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     86/99   0.009593    0.5676         0    0.7854: 100%|██████████| 67/67 [01:19<00:00,  1.19s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:04<00:00,  1.08it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.60s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.03s).
Accumulating evaluation results...
DONE (t=0.25s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.288
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.558
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.258
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.189
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.505
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.286
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.412
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.332
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.601
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.650
Results saved to runs/train/gpu1
Epoch: 86 | mAP@0.5: 0.5576701311679166 | mAP@0.50:0.95: 0.28750477443085926

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     87/99   0.008567     0.568         0    0.7808: 100%|██████████| 67/67 [01:20<00:00,  1.21s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     88/99   0.007607    0.5635         0      0.78: 100%|██████████| 67/67 [01:20<00:00,  1.21s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     89/99   0.006714    0.5615         0    0.7723: 100%|██████████| 67/67 [01:20<00:00,  1.21s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:04<00:00,  1.07it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.53s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.02s).
Accumulating evaluation results...
DONE (t=0.19s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.295
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.566
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.279
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.198
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.510
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.606
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.078
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.291
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.411
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.614
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.665
Results saved to runs/train/gpu1
Epoch: 89 | mAP@0.5: 0.5656820240972437 | mAP@0.50:0.95: 0.2946301206102219

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     90/99    0.00589    0.5645         0    0.7752: 100%|██████████| 67/67 [01:20<00:00,  1.21s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     91/99   0.005134    0.5605         0    0.7699: 100%|██████████| 67/67 [01:20<00:00,  1.21s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     92/99   0.004448    0.5618         0    0.7704: 100%|██████████| 67/67 [01:17<00:00,  1.16s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.76it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.98s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.297
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.564
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.281
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.519
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.079
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.295
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.415
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.332
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.615
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.650
Results saved to runs/train/gpu1
Epoch: 92 | mAP@0.5: 0.5637698330906846 | mAP@0.50:0.95: 0.2968609862129802

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     93/99   0.003833     0.557         0    0.7689: 100%|██████████| 67/67 [01:09<00:00,  1.03s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     94/99   0.003288    0.5651         0    0.7693: 100%|██████████| 67/67 [01:09<00:00,  1.03s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     95/99   0.002815    0.5531         0    0.7664: 100%|██████████| 67/67 [01:09<00:00,  1.04s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.88it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.28s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.99s).
Accumulating evaluation results...
DONE (t=0.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.301
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.571
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.079
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.297
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.620
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.700
Results saved to runs/train/gpu1
Epoch: 95 | mAP@0.5: 0.5712028290786884 | mAP@0.50:0.95: 0.3005675807700216

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     96/99   0.002414     0.555         0    0.7714: 100%|██████████| 67/67 [01:09<00:00,  1.03s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     97/99   0.002085     0.561         0    0.7695: 100%|██████████| 67/67 [01:09<00:00,  1.03s/it]

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     98/99    0.00183    0.5579         0    0.7728: 100%|██████████| 67/67 [01:09<00:00,  1.03s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.84it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.99s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.302
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.574
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.286
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.518
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.650
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.080
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.296
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.419
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.613
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.710
Results saved to runs/train/gpu1
Epoch: 98 | mAP@0.5: 0.574033525228527 | mAP@0.50:0.95: 0.3021774878493339

     Epoch        lr  iou_loss  dfl_loss  cls_loss
     99/99   0.001647    0.5558         0    0.7699: 100%|██████████| 67/67 [01:09<00:00,  1.03s/it]
Inferencing model in train datasets.: 100%|███████████████████████████| 5/5 [00:02<00:00,  1.92it/s]

Evaluating speed.

Evaluating mAP by pycocotools.
Saving runs/train/gpu1/predictions.json...
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Loading and preparing results...
DONE (t=0.27s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=0.98s).
Accumulating evaluation results...
DONE (t=0.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.300
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.576
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.516
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.587
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.080
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.295
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.418
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.613
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.645
Results saved to runs/train/gpu1
Epoch: 99 | mAP@0.5: 0.5763539720494121 | mAP@0.50:0.95: 0.30002021336048124

Training completed in 2.073 hours.